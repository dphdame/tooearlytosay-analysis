#!/usr/bin/env python3
"""
Vulnerability Index Calculation

Author: V Cholette
Purpose: Calculate food insecurity vulnerability index for Santa Clara County census tracts

Inputs:
- ACS demographic data (from script 01)
- Census TIGER/Line shapefiles (see data/README.md)

Outputs:
- Vulnerability index scores and quintiles
- Spatial dataset ready for mapping

Usage:
    python 02_calculate_vulnerability_index.py
"""

import pandas as pd
import geopandas as gpd
import numpy as np
from pathlib import Path
import logging
import json
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# Configuration
# ============================================================================

# Use relative paths from project root
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"

# Input files
ACS_FILE = DATA_DIR / "raw" / "acs_tracts_scc.csv"
SHAPEFILE = DATA_DIR / "external" / "shapefiles" / "tl_2023_06_tract.shp"

# Output files
OUTPUT_DIR = DATA_DIR / "processed"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

OUTPUT_CSV = OUTPUT_DIR / "vulnerability_index.csv"
OUTPUT_GEOJSON = OUTPUT_DIR / "vulnerability_index.geojson"

# Vulnerability index weights (must sum to 1.0)
WEIGHTS = {
    'poverty_rate': 0.40,       # Economic hardship
    'snap_rate': 0.35,          # Food assistance need
    'pop_density_inv': 0.25     # Geographic isolation (inverse of density)
}

# Santa Clara County FIPS
COUNTY_FIPS = "085"


# ============================================================================
# Data Loading
# ============================================================================

def load_acs_data(filepath):
    """Load ACS demographic data and calculate rates."""
    logger.info(f"Loading ACS data from {filepath}")

    if not filepath.exists():
        raise FileNotFoundError(
            f"ACS data not found: {filepath}\n"
            "Run script 01_acquire_census_data.py first."
        )

    df = pd.read_csv(filepath)

    # Create GEOID if not present
    if 'GEOID' not in df.columns:
        df['GEOID'] = (df['state'].astype(str).str.zfill(2) +
                       df['county'].astype(str).str.zfill(3) +
                       df['tract'].astype(str).str.zfill(6))

    # Convert numeric columns
    numeric_cols = ['B01003_001E', 'B17001_001E', 'B17001_002E',
                    'B22003_001E', 'B22003_002E']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # Calculate rates
    df['poverty_rate'] = df['B17001_002E'] / df['B17001_001E']
    df['snap_rate'] = df['B22003_002E'] / df['B22003_001E']
    df['total_population'] = df['B01003_001E']
    df['total_households'] = df['B22003_001E']

    # Handle division by zero
    df['poverty_rate'] = df['poverty_rate'].fillna(0)
    df['snap_rate'] = df['snap_rate'].fillna(0)

    logger.info(f"Loaded {len(df)} tracts")
    logger.info(f"Poverty rate range: {df['poverty_rate'].min():.3f} - {df['poverty_rate'].max():.3f}")
    logger.info(f"SNAP rate range: {df['snap_rate'].min():.3f} - {df['snap_rate'].max():.3f}")

    return df


def load_shapefile(filepath, county_fips=COUNTY_FIPS):
    """Load Census tract boundaries and calculate areas."""
    logger.info(f"Loading shapefile from {filepath}")

    if not filepath.exists():
        raise FileNotFoundError(
            f"Shapefile not found: {filepath}\n"
            "Download from Census TIGER/Line. See data/README.md for instructions."
        )

    gdf = gpd.read_file(filepath)

    # Filter to Santa Clara County
    gdf = gdf[gdf['COUNTYFP'] == county_fips].copy()
    logger.info(f"Filtered to {len(gdf)} Santa Clara County tracts")

    # Calculate area in square miles (reproject to California Albers)
    gdf_projected = gdf.to_crs("EPSG:3310")
    gdf['area_sqmi'] = gdf_projected.geometry.area / 2589988.11  # sq meters to sq miles

    # Keep relevant columns
    gdf = gdf[['GEOID', 'NAMELSAD', 'area_sqmi', 'geometry']]

    logger.info(f"Area range: {gdf['area_sqmi'].min():.2f} - {gdf['area_sqmi'].max():.2f} sq mi")

    return gdf


# ============================================================================
# Index Calculation
# ============================================================================

def merge_datasets(acs_df, geo_df):
    """Merge ACS and geographic data."""
    logger.info("Merging datasets...")

    # Start with geographic data
    merged = geo_df.merge(
        acs_df[['GEOID', 'poverty_rate', 'snap_rate', 'total_population', 'total_households']],
        on='GEOID',
        how='left'
    )

    # Calculate population density
    merged['pop_density'] = merged['total_population'] / merged['area_sqmi']

    # Check for missing data
    missing = merged['poverty_rate'].isna().sum()
    if missing > 0:
        logger.warning(f"{missing} tracts missing demographic data")

    logger.info(f"Merged dataset: {len(merged)} tracts")

    return merged


def calculate_vulnerability_index(df, weights=WEIGHTS):
    """
    Calculate composite vulnerability index.

    Components are min-max normalized to [0, 1], then weighted.
    Higher index = more vulnerable.
    """
    logger.info("Calculating vulnerability index...")
    df = df.copy()

    # Filter to tracts with complete data
    complete = df[['poverty_rate', 'snap_rate', 'pop_density']].notna().all(axis=1)
    logger.info(f"Complete data for {complete.sum()} of {len(df)} tracts")

    # Min-max normalization
    def normalize(series):
        min_val = series.min()
        max_val = series.max()
        if max_val == min_val:
            return pd.Series(0, index=series.index)
        return (series - min_val) / (max_val - min_val)

    # Normalize components
    df['poverty_norm'] = normalize(df['poverty_rate'])
    df['snap_norm'] = normalize(df['snap_rate'])

    # Population density: INVERSE (lower density = more vulnerable/isolated)
    df['density_norm'] = normalize(df['pop_density'])
    df['density_inv_norm'] = 1 - df['density_norm']

    # Calculate weighted composite
    df['vulnerability_index'] = (
        weights['poverty_rate'] * df['poverty_norm'] +
        weights['snap_rate'] * df['snap_norm'] +
        weights['pop_density_inv'] * df['density_inv_norm']
    )

    # Classify into quintiles
    df['vulnerability_quintile'] = pd.qcut(
        df['vulnerability_index'].rank(method='first'),
        q=5,
        labels=['Q1_Lowest', 'Q2_Low', 'Q3_Medium', 'Q4_High', 'Q5_Highest']
    )

    # Summary statistics
    logger.info(f"Vulnerability index range: {df['vulnerability_index'].min():.3f} - "
                f"{df['vulnerability_index'].max():.3f}")

    quintile_counts = df['vulnerability_quintile'].value_counts().sort_index()
    logger.info("Quintile distribution:")
    for q, count in quintile_counts.items():
        logger.info(f"  {q}: {count} tracts")

    return df


# ============================================================================
# Main Execution
# ============================================================================

def main():
    """Main execution function."""
    print("=" * 60)
    print("VULNERABILITY INDEX CALCULATION")
    print("Santa Clara County, California")
    print("=" * 60)

    # Load data
    acs_df = load_acs_data(ACS_FILE)
    geo_df = load_shapefile(SHAPEFILE)

    # Merge datasets
    merged = merge_datasets(acs_df, geo_df)

    # Calculate vulnerability index
    result = calculate_vulnerability_index(merged)

    # Save outputs
    print("\n" + "=" * 60)
    print("SAVING OUTPUTS")
    print("=" * 60)

    # 1. CSV (without geometry)
    csv_cols = ['GEOID', 'NAMELSAD', 'area_sqmi', 'total_population',
                'total_households', 'poverty_rate', 'snap_rate', 'pop_density',
                'vulnerability_index', 'vulnerability_quintile']
    result[csv_cols].to_csv(OUTPUT_CSV, index=False)
    print(f"\n✓ CSV saved: {OUTPUT_CSV}")

    # 2. GeoJSON (with geometry)
    result.to_file(OUTPUT_GEOJSON, driver='GeoJSON')
    print(f"✓ GeoJSON saved: {OUTPUT_GEOJSON}")

    # 3. Summary statistics
    summary = {
        "calculation_date": datetime.utcnow().isoformat(),
        "tract_count": len(result),
        "weights": WEIGHTS,
        "statistics": {
            "poverty_rate": {
                "min": float(result['poverty_rate'].min()),
                "max": float(result['poverty_rate'].max()),
                "mean": float(result['poverty_rate'].mean()),
                "median": float(result['poverty_rate'].median())
            },
            "snap_rate": {
                "min": float(result['snap_rate'].min()),
                "max": float(result['snap_rate'].max()),
                "mean": float(result['snap_rate'].mean()),
                "median": float(result['snap_rate'].median())
            },
            "vulnerability_index": {
                "min": float(result['vulnerability_index'].min()),
                "max": float(result['vulnerability_index'].max()),
                "mean": float(result['vulnerability_index'].mean()),
                "median": float(result['vulnerability_index'].median())
            }
        },
        "quintile_distribution": result['vulnerability_quintile'].value_counts().to_dict()
    }

    summary_file = OUTPUT_DIR / "vulnerability_summary.json"
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    print(f"✓ Summary saved: {summary_file}")

    # Print summary
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print(f"\nTracts analyzed: {len(result)}")
    print(f"Weights: poverty={WEIGHTS['poverty_rate']}, "
          f"snap={WEIGHTS['snap_rate']}, "
          f"isolation={WEIGHTS['pop_density_inv']}")
    print(f"\nVulnerability Index:")
    print(f"  Range: {result['vulnerability_index'].min():.3f} - "
          f"{result['vulnerability_index'].max():.3f}")
    print(f"  Mean: {result['vulnerability_index'].mean():.3f}")

    print("\nQuintile Distribution:")
    for q in ['Q1_Lowest', 'Q2_Low', 'Q3_Medium', 'Q4_High', 'Q5_Highest']:
        count = (result['vulnerability_quintile'] == q).sum()
        print(f"  {q}: {count} tracts")

    print(f"\nNext step: python 03_create_vulnerability_map.py")


if __name__ == "__main__":
    main()
